cores=$1

numactl -C 0-$((cores-1)) -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --cores_per_instance 4 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 20 --perf_run_iters 100 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --total_cores $cores --use_jit --use_share_weight 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_latency_avx_fp32_bs_1_real_20220510235153_1_20220510235154_instance_0_cores_0-55.log
