numactl -C 0-3 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_0_cores_0-3.log &

numactl -C 4-7 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_1_cores_4-7.log &

numactl -C 8-11 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_2_cores_8-11.log &

numactl -C 12-15 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_3_cores_12-15.log &

numactl -C 16-19 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_4_cores_16-19.log &

numactl -C 20-23 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_5_cores_20-23.log &

numactl -C 24-27 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_6_cores_24-27.log &

numactl -C 28-31 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_7_cores_28-31.log &

numactl -C 32-35 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_8_cores_32-35.log &

numactl -C 36-39 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_9_cores_36-39.log &

numactl -C 40-43 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_10_cores_40-43.log &

numactl -C 44-47 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_11_cores_44-47.log &

numactl -C 48-51 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_12_cores_48-51.log &

numactl -C 52-55 -m 0 /root/anaconda3/envs/pytorch/bin/python3 -u /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/question-answering/run_squad.py --benchmark --bf16 --do_eval --do_lower_case --doc_stride 128 --int8_config /root/workspace/benchmark/pytorch_model/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --learning_rate 3e-5 --max_seq_length 384 --model_name_or_path /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint --model_type bert --num_train_epochs 2.0 --output_dir ./tmp --per_gpu_eval_batch_size 1 --perf_begin_iter 15 --perf_run_iters 330 --predict_file /home/dataset/pytorch/bert/enwiki-20200101/bert_large_mlperf_checkpoint/checkpoint/dev-v1.1.json --tokenizer_name bert-large-uncased-whole-word-masking-finetuned-squad --use_jit 2>&1 | tee /home/dl_boost/log/pytorch/instance_logs/bert_large/bert_large_log_inference_throughput_amx_bfloat16_bs_1_real_20220505225551_1_20220505225552_instance_13_cores_52-55.log &
